# 07. Testing & Evals：把 agent 开发从“调感觉”变成“调系统”

## 1) 为什么 agent 必须评测驱动
普通应用可以靠手工 spot-check，agent 不行。

原因：
- agent 是多步系统，错误会级联放大。
- 一次成功不代表策略稳定。
- 工具、提示、检索、模型会相互耦合，必须系统观测。

`Evals（评测）` 不是锦上添花，而是 agent 工程化的入口门槛。

## 2) 评测对象应该包含什么
不要只测“最终答案对不对”，还要测过程质量：
- 任务成功率（task success rate）
- 工具调用有效率（tool call effectiveness）
- 错误率（tool errors / invalid args）
- 延迟（latency）
- token 成本（cost）

这决定了你能否定位“是模型问题、工具问题、还是上下文问题”。

## 3) 构建评测集（Eval Set）的方法

### A. 来源要真实（Realistic Tasks）
从真实用户任务和历史失败案例抽样，而不是“教科书式简单题”。

### B. 难度要分层（Tiered Difficulty）
- L1：单步可解
- L2：多步协作
- L3：高不确定 + 外部依赖

### C. 需要保留 holdout 集
防止针对训练集过拟合，确保改动在未见任务上也有效。

## 4) Verifier 设计：自动化判分的关键
验证器可从简到繁：
- 精确匹配（exact match）
- 规则匹配（schema/constraints）
- 稳健比较（容忍格式差异）
- 模型评审（用于软性指标）

核心原则：
- 先做稳定硬规则，再考虑模糊评估。

## 5) 失败分析框架（Failure Taxonomy）
当 agent 失败时，优先按以下维度归因：
1. `Context failure`：没拿到关键信息。
2. `Tool failure`：选错工具或参数。
3. `Reasoning failure`：推理路径错误。
4. `Verification failure`：明明错了却没被拦住。

这个分类比“模型不行”更可操作。

## 6) 迭代闭环（Eval-Driven Iteration）
建议固定迭代节奏：
1. 跑基线评测（baseline）
2. 分析失败样本
3. 只改一个变量（工具描述/规则/检索结构）
4. 回归评测 + holdout 验证
5. 记录变更与收益

这个节奏能避免“改了一堆，不知道哪条有效”。

## 7) 工具评测的特别建议
在工具设计文档中同步维护：
- 典型调用样例
- 反例（常见误用）
- 错误响应标准
- 成本约束（分页、截断、过滤）

因为工具接口本身就是 prompt 的一部分。

## 8) 常见误区
- 只看终局准确率，不看过程指标。
- 评测集过于简单，线上一打就崩。
- 每次改很多项，无法定位收益来源。
- 没有 holdout，优化后看似进步实则过拟合。

## 9) 你应该带走的结论
- 评测是 agent 团队的“共同语言”，能把争论变成可验证实验。
- 最有效的改进通常来自失败样本，而非成功样本。
- 想要稳定迭代，必须把“评测管线”当产品能力建设。

## Checklist
- 是否建立了覆盖真实场景的多层评测集？
- 指标是否覆盖成功率、成本、时延、工具错误？
- 是否有明确的失败归因框架？
- 每次迭代是否坚持单变量改动和回归验证？
